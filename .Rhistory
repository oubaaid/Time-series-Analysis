ets_rmse <- NA
}
# Additive Holt-Winters
if (length(test_power) == length(hw_add_forecast$mean)) {
hw_add_rmse <- calculate_rmse(test_power, hw_add_forecast$mean)
} else {
hw_add_rmse <- NA
}
library(forecast)
# Define the correct forecasting horizon based on test_power length
forecast_horizon <- length(test_power)
# Apply the models with the correct forecast horizon
# Exponential Smoothing (ETS)
ets_model <- ets(train_power)
ets_forecast <- forecast(ets_model, h = forecast_horizon)
# Additive Holt-Winters
hw_add_model <- ets(train_power, model = "AAA")
# Load necessary libraries
library(readxl)
library(dplyr)
library(lubridate)
library(ggplot2)
# Load the data
file_path <- "path_to_your_file.xlsx"  # replace with your file path
data <- read_excel(file_path)
# Load necessary libraries
library(readxl)
library(dplyr)
library(lubridate)
library(ggplot2)
# Load the data
file_path <- "C:/Users/ou_ba/Downloads/Elec-train.xlsx"  # replace with your file path
data <- read_excel(file_path)
# Rename the columns
colnames(data) <- c("Timestamp", "Power", "Temp")
# Convert Timestamp to the correct format
data$Timestamp <- as.POSIXct(data$Timestamp, format="%m/%d/%Y %H:%M")
# Remove the first line of data
data <- data[-1, ]
# Convert the relevant Power rows to NA
data$Power[4603:4613] <- NA
# Rename the columns
colnames(data) <- c("Timestamp", "Power", "Temp")
# Convert Timestamp to the correct format
data$Timestamp <- as.POSIXct(data$Timestamp, format="%m/%d/%Y %H:%M")
# Remove the first line of data
data <- data[-1, ]
data
# Load necessary libraries
library(readxl)
library(dplyr)
library(lubridate)
library(zoo)
library(ggplot2)
library(forecast)
library(tseries)
library(nnet)
# Step 1: Load the data
file_path <- "C:\Users\ou_ba\Downloads\Elec-train.xlsx"  # replace with your file path
# Step 1: Load the data
file_path <- "C:/Users/ou_ba/Downloads/Elec-train.xlsx"  # replace with your file path
data <- read_excel(file_path)
# Rename the columns
colnames(data) <- c("Timestamp", "Power", "Temp")
# Convert Timestamp to the correct format
data$Timestamp <- as.POSIXct(data$Timestamp, format="%m/%d/%Y %H:%M")
# Step 2: Data Cleaning
# Remove the first line of data
data <- data[-1, ]
# Convert the relevant Power rows to NA
data$Power[4603:4613] <- NA
# Convert the relevant Power rows to NA
data$Power[4603:4613] <- NA
nrow(data)
# Load necessary libraries
library(readxl)
library(dplyr)
library(lubridate)
library(ggplot2)
# Load the data
file_path <- "C:/Users/ou_ba/Downloads/2023-11-Elec-train.xlsx"
# replace with your file path
data <- read_excel(file_path)
# Rename the columns
colnames(data) <- c("Timestamp", "Power", "Temp")
# Convert Timestamp to the correct format
data$Timestamp <- as.POSIXct(data$Timestamp, format="%m/%d/%Y %H:%M")
# Remove the first line of data
data <- data[-1, ]
# Convert the relevant Power rows to NA
data$Power[4603:4613] <- NA
# Prepare the surrounding data for interpolation (40 rows before and after)
before_data <- data[(4603-40):(4603-1), ]
after_data <- data[(4613+1):(4613+40), ]
# Combine before and after data
surrounding_data <- rbind(before_data, after_data)
# Fit a polynomial model (degree = 3 as an example)
model <- lm(Power ~ poly(1:nrow(surrounding_data), 3), data = surrounding_data)
# Predict the missing values
predicted_values <- predict(model, newdata = data.frame('1:nrow(surrounding_data)' = 4603:4613))
# Fill the missing values in the original data
data$Power[4603:4613] <- predicted_values
# Step 3: Data Visualization
# Plot Power over time
ggplot(data, aes(x = Timestamp, y = Power)) +
geom_line() +
labs(title = "Power Consumption Over Time", x = "Time", y = "Power (kW)")
# Plot Temp over time
ggplot(data, aes(x = Timestamp, y = Temp)) +
geom_line(color = 'red') +
labs(title = "Temperature Over Time", x = "Time", y = "Temperature (Â°C)")
# Step 4: Data Splitting
train_size <- floor(0.8 * nrow(data))
train_data <- data[1:train_size, ]
test_data <- data[(train_size + 1):nrow(data), ]
library(forecast)
library(nnet)
# Prepare the training and test sets (just the Power column initially)
train_power <- ts(train_data$Power, frequency = 96) # 96 because data is collected every 15 minutes (24*4)
test_power <- ts(test_data$Power, start = c(floor(train_size/96)+1, 1), frequency = 96)
# 1. Exponential Smoothing (ETS)
ets_model <- ets(train_power)
ets_forecast <- forecast(ets_model, h = length(test_power))
ets_rmse <- sqrt(mean((test_power - ets_forecast$mean)^2))
library(forecast)
# Use stlf() to handle high-frequency seasonality
stl_ets_model <- stlf(train_power, method = "ets")
stl_ets_forecast <- forecast(stl_ets_model, h = length(test_power))
library(forecast)
# Use stlf() to handle high-frequency seasonality
stl_ets_model <- stlf(train_power, method = "ets")
stl_ets_forecast <- forecast(stl_ets_model, h = length(test_power))
# Plot residuals
checkresiduals(stl_ets_model)
# Summary of the model
summary(stl_ets_model)
library(forecast)
library(nnet)
# Assuming the ets_model has already been created with your adjusted frequency:
ets_forecast <- forecast(ets_model, h = length(test_power))
ets_rmse <- sqrt(mean((test_power - ets_forecast$mean)^2))
# 2. Additive Seasonal Holt-Winters (using adjusted ETS model)
hw_add_model <- ets(train_power, model = "AAA")
# Apply stlf() with ETS
stlf_ets_model <- stlf(train_power, method = "ets")
stlf_ets_forecast <- forecast(stlf_ets_model, h = length(test_power))
# Apply stlf() with ARIMA
stlf_arima_model <- stlf(train_power, method = "arima")
stlf_arima_forecast <- forecast(stlf_arima_model, h = length(test_power))
# Apply stlf() with ARIMA and specify the forecast horizon
stlf_arima_model <- stlf(train_power, method = "arima", h = length(test_power))
# The forecasts are already computed within stlf(), no need for forecast() again
stlf_arima_forecast <- stlf_arima_model
# Calculate RMSE
stlf_arima_rmse <- sqrt(mean((test_power - stlf_arima_forecast$mean)^2, na.rm = TRUE))
# Print the RMSE
print(stlf_arima_rmse)
# Apply stlf() with ETS and specify the forecast horizon
stlf_ets_model <- stlf(train_power, method = "ets", h = length(test_power))
# RMSE Calculation
stlf_ets_rmse <- sqrt(mean((test_power - stlf_ets_model$mean)^2, na.rm = TRUE))
# Print the RMSE
print(stlf_ets_rmse)
# Apply stlf() with Theta method and specify the forecast horizon
stlf_theta_model <- stlf(train_power, method = "theta", h = length(test_power))
# Apply STL decomposition manually with specified horizon
stl_decomp <- stl(train_power, s.window = "periodic")
# Fit NNETAR to the remainder component
nnet_model <- nnetar(stl_decomp$time.series[, "remainder"])
stlf_nnet_forecast <- forecast(nnet_model, h = length(test_power))
# Combine with seasonal and trend components
stlf_nnet_combined_forecast <- stl_decomp$time.series[, "seasonal"] +
stl_decomp$time.series[, "trend"] +
stlf_nnet_forecast$mean
# RMSE Calculation
stlf_nnet_rmse <- sqrt(mean((test_power - stlf_nnet_combined_forecast)^2, na.rm = TRUE))
View(stlf_nnet_forecast)
View(surrounding_data)
View(test_data)
library(forecast)
library(nnet)
# Prepare the training and test sets (just the Power column initially)
train_power <- ts(train_data$Power, frequency = 96) # 96 because data is collected every 15 minutes (24*4)
test_power <- ts(test_data$Power, start = c(floor(train_size/96)+1, 1), frequency = 96)
# 1. Exponential Smoothing (ETS)
ets_model <- ets(train_power)
ets_forecast <- forecast(ets_model, h = length(test_power))
ets_rmse <- sqrt(mean((test_power - ets_forecast$mean)^2))
# 2. Additive Seasonal Holt-Winters
hw_add_model <- hw(train_power, seasonal = "additive")
# 7. Neural Networks (NNET)
nnet_model <- nnetar(train_power)
nnet_forecast <- forecast(nnet_model, h = length(test_power))
nnet_rmse <- sqrt(mean((test_power - nnet_forecast$mean)^2))
# Check for NA values in test_power
any(is.na(test_power))
# Check for NA values in each forecast model
any(is.na(stlf_ets_model$mean))
any(is.na(stlf_arima_model$mean))
any(is.na(stlf_theta_model$mean))
# 1. Exponential Smoothing (ETS)
ets_model <- ets(train_power)
ets_forecast <- forecast(ets_model, h = length(test_power))
ets_rmse <- sqrt(mean((test_power - ets_forecast$mean)^2, na.rm = TRUE))
# 2. Additive Seasonal Holt-Winters
hw_add_model <- hw(train_power, seasonal = "additive")
# 3. Multiplicative Seasonal Holt-Winters
hw_mul_model <- hw(train_power, seasonal = "multiplicative")
# 4. Additive Seasonal Holt-Winters with Trend
hw_add_trend_model <- HoltWinters(train_power, seasonal = "additive")
hw_add_trend_forecast <- forecast(hw_add_trend_model, h = length(test_power))
hw_add_trend_rmse <- sqrt(mean((test_power - hw_add_trend_forecast$mean)^2, na.rm = TRUE))
# 5. Multiplicative Seasonal Holt-Winters with Trend
hw_mul_trend_model <- HoltWinters(train_power, seasonal = "multiplicative")
hw_mul_trend_forecast <- forecast(hw_mul_trend_model, h = length(test_power))
hw_mul_trend_rmse <- sqrt(mean((test_power - hw_mul_trend_forecast$mean)^2, na.rm = TRUE))
# 6. Auto ARIMA
arima_model <- auto.arima(train_power)
arima_forecast <- forecast(arima_model, h = length(test_power))
arima_rmse <- sqrt(mean((test_power - arima_forecast$mean)^2, na.rm = TRUE))
# 7. Neural Networks (NNET)
nnet_model <- nnetar(train_power)
nnet_forecast <- forecast(nnet_model, h = length(test_power))
nnet_rmse <- sqrt(mean((test_power - nnet_forecast$mean)^2, na.rm = TRUE))
# 8. SARIMA (Seasonal ARIMA)
sarima_model <- Arima(train_power, order = c(1,1,1), seasonal = c(1,1,1))
# Print RMSE results for comparison
rmse_results <- data.frame(
Model = c("Exponential Smoothing", "Additive Holt-Winters",
"Multiplicative Holt-Winters", "Additive HW with Trend",
"Multiplicative HW with Trend", "Auto ARIMA",
"Neural Network", "SARIMA"),
RMSE = c(ets_rmse, hw_add_rmse, hw_mul_rmse, hw_add_trend_rmse,
hw_mul_trend_rmse, arima_rmse, nnet_rmse, sarima_rmse)
)
print(rmse_results)
# Print RMSE results for comparison
rmse_results <- data.frame(
Model = c("Exponential Smoothing", "Additive Holt-Winters",
"Multiplicative Holt-Winters", "Additive HW with Trend",
"Multiplicative HW with Trend", "Auto ARIMA",
"Neural Network", "SARIMA"),
RMSE = c(ets_rmse, hw_add_trend_rmse,
hw_mul_trend_rmse, arima_rmse, nnet_rmse)
)
# Print RMSE results for comparison
rmse_results <- data.frame(
Model = c("Exponential Smoothing", "Additive HW with Trend",
"Multiplicative HW with Trend", "Auto ARIMA",
"Neural Network"),
RMSE = c(ets_rmse, hw_add_trend_rmse,
hw_mul_trend_rmse, arima_rmse, nnet_rmse)
)
print(rmse_results)
# If you want to visualize the RMSE values by each model, a bar plot might be more appropriate:
ggplot(rmse_results, aes(x = Model, y = RMSE, fill = Model)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "RMSE Values by Model",
x = "Model",
y = "RMSE") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
ets_forecast
# View forecasted values (mean forecasts)
print(ets_forecast$mean)
# View confidence intervals (lower and upper bounds)
print(ets_forecast$lower)
print(ets_forecast$upper)
# Plot the forecast to visualize it
plot(ets_forecast)
library(ggplot2)
# Convert forecast to a data frame for ggplot2
forecast_df <- data.frame(
Date = time(ets_forecast$mean),
Forecast = ets_forecast$mean,
Lower80 = ets_forecast$lower[, "80%"],
Upper80 = ets_forecast$upper[, "80%"]
)
# Plot with ggplot2
ggplot(forecast_df, aes(x = Date, y = Forecast)) +
geom_line(color = "blue") +
geom_ribbon(aes(ymin = Lower80, ymax = Upper80), alpha = 0.2) +
labs(title = "ETS Forecast", x = "Date", y = "Forecast") +
theme_minimal()
library(forecast)
# Fit ETS model
ets_model <- ets(train_power)
# Generate forecasts for the length of the test period
ets_forecast <- forecast(ets_model, h = length(test_power))
# Plot the forecast with axis titles
plot(ets_forecast,
main = "ETS Forecast",  # Main title
xlab = "Time",           # X-axis title
ylab = "Forecasted Values"  # Y-axis title
)
# Plot residuals
checkresiduals(ets_model)
# Summary of the model
summary(ets_model)
# Assuming `train_data` and `test_data` are data frames containing both `Power` and `Temp`
train_power <- ts(train_data$Power, frequency = 96)  # 96 for 15-minute intervals
train_temp <- ts(train_data$Temp, frequency = 96)
test_power <- ts(test_data$Power, start = c(floor(train_size/96) + 1, 1), frequency = 96)
test_temp <- ts(test_data$Temp, start = c(floor(train_size/96) + 1, 1), frequency = 96)
# Auto ARIMA with Temp as a regressor
arima_model <- auto.arima(train_power, xreg = train_temp)
arima_forecast <- forecast(arima_model, xreg = test_temp, h = length(test_power))
arima_rmse <- sqrt(mean((test_power - arima_forecast$mean)^2, na.rm = TRUE))
# Neural Network with Temp as a regressor
nnet_model <- nnetar(train_power, xreg = train_temp)
nnet_forecast <- forecast(nnet_model, xreg = test_temp, h = length(test_power))
nnet_rmse <- sqrt(mean((test_power - nnet_forecast$mean)^2, na.rm = TRUE))
# SARIMA with Temp as a regressor
sarima_model <- Arima(train_power, order = c(1,1,1), seasonal = c(1,1,1), xreg = train_temp)
# Auto ARIMA with Temp as an exogenous regressor
arima_model <- auto.arima(train_power, xreg = train_temp)
arima_forecast <- forecast(arima_model, xreg = test_temp)
arima_rmse <- sqrt(mean((test_power - arima_forecast$mean)^2, na.rm = TRUE))
# 1. Exponential Smoothing (ETS)
ets_model <- ets(train_power)
ets_forecast <- forecast(ets_model, h = length(test_power))
ets_rmse <- sqrt(mean((test_power - ets_forecast$mean)^2, na.rm = TRUE))
# 2. Additive Seasonal Holt-Winters
hw_add_model <- hw(train_power, seasonal = "additive")
# 1. Auto ARIMA with External Regressor
arima_model <- auto.arima(train_power, xreg = train_temp)
arima_forecast <- forecast(arima_model, xreg = test_temp)
arima_rmse <- sqrt(mean((test_power - arima_forecast$mean)^2, na.rm = TRUE))
# Print RMSE results for comparison
rmse_results <- data.frame(
Model = c("Exponential Smoothing", "Additive Holt-Winters",
"Multiplicative Holt-Winters", "Auto ARIMA",
"Neural Network"),
RMSE = c(NA, NA, NA, arima_rmse, NA)
)
print(rmse_results)
# Bar plots of RMSE values by each model:
ggplot(rmse_results, aes(x = Model, y = RMSE, fill = Model)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "RMSE Values by Model",
x = "Model",
y = "RMSE") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
# Print forecasted values
print(arima_forecast$mean)
# Print confidence intervals
print(arima_forecast$lower)
print(arima_forecast$upper)
# Plot the forecast
plot(arima_forecast)
# Plot the forecast with axis titles
plot(arima_forecast,
main = "ARIMA Forecast with External Regressor",  # Main title
xlab = "Time",                                   # X-axis title
ylab = "Forecasted Power Values"                 # Y-axis title
)
# Print RMSE results for comparison
rmse_results <- data.frame(
Model = c("Exponential Smoothing", "Additive Holt-Winters",
"Multiplicative Holt-Winters", "Auto ARIMA",
"Neural Network"),
RMSE = c(NA, NA, NA, arima_rmse, NA)
)
# Bar plots of RMSE values by each model:
ggplot(rmse_results, aes(x = Model, y = RMSE, fill = Model)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "RMSE Values by Model",
x = "Model",
y = "RMSE") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
library(nnet)
library(ggplot2)
# Prepare the data
train_data_nn <- data.frame(
Power = train_data$Power,
Temp = train_data$Temp
)
test_data_nn <- data.frame(
Power = test_data$Power,
Temp = test_data$Temp
)
# Fit neural network model
nn_model <- nnet(Power ~ Temp, data = train_data_nn, size = 10, linout = TRUE)
# Forecast using the neural network model
nn_predictions <- predict(nn_model, newdata = test_data_nn)
# Calculate RMSE
nn_rmse <- sqrt(mean((test_data$Power - nn_predictions)^2, na.rm = TRUE))
# Print RMSE results
rmse_results <- data.frame(
Model = c("Exponential Smoothing", "Additive Holt-Winters",
"Multiplicative Holt-Winters", "Auto ARIMA",
"Neural Network"),
RMSE = c(NA, NA, NA, arima_rmse, nn_rmse)
)
print(rmse_results)
# Plot RMSE results
ggplot(rmse_results, aes(x = Model, y = RMSE, fill = Model)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "RMSE Values by Model",
x = "Model",
y = "RMSE") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
library(forecast)
library(ggplot2)
# Prepare the training and test sets
train_power <- ts(train_data$Power, frequency = 96)
test_power <- ts(test_data$Power, start = c(floor(train_size/96)+1, 1), frequency = 96)
train_temp <- ts(train_data$Temp, frequency = 96)
test_temp <- ts(test_data$Temp, start = c(floor(train_size/96)+1, 1), frequency = 96)
# 1. Auto ARIMA with External Regressor
arima_model <- auto.arima(train_power, xreg = train_temp)
arima_forecast <- forecast(arima_model, xreg = test_temp)
arima_rmse <- sqrt(mean((test_power - arima_forecast$mean)^2, na.rm = TRUE))
# 2. Neural Networks (NNET)
nnet_model <- nnetar(train_power)
nnet_forecast <- forecast(nnet_model, h = length(test_power))
nnet_rmse <- sqrt(mean((test_power - nnet_forecast$mean)^2, na.rm = TRUE))
# Print RMSE results for comparison
rmse_results <- data.frame(
Model = c("Auto ARIMA", "Neural Network"),
RMSE = c(arima_rmse, nnet_rmse)
)
print(rmse_results)
# Convert forecast results to data frames for ggplot2
arima_forecast_df <- data.frame(
Date = time(arima_forecast$mean),
Forecast = arima_forecast$mean,
Lower80 = arima_forecast$lower[, "80%"],
Upper80 = arima_forecast$upper[, "80%"]
)
nnet_forecast_df <- data.frame(
Date = time(nnet_forecast$mean),
Forecast = nnet_forecast$mean,
Lower80 = nnet_forecast$lower[, "80%"],
Upper80 = nnet_forecast$upper[, "80%"]
)
library(forecast)
library(ggplot2)
# Prepare the training and test sets
train_power <- ts(train_data$Power, frequency = 96)
test_power <- ts(test_data$Power, start = c(floor(train_size/96)+1, 1), frequency = 96)
train_temp <- ts(train_data$Temp, frequency = 96)
test_temp <- ts(test_data$Temp, start = c(floor(train_size/96)+1, 1), frequency = 96)
# 1. Auto ARIMA with External Regressor
arima_model <- auto.arima(train_power, xreg = train_temp)
arima_forecast <- forecast(arima_model, xreg = test_temp)
arima_rmse <- sqrt(mean((test_power - arima_forecast$mean)^2, na.rm = TRUE))
# 2. Neural Networks (NNET)
nnet_model <- nnetar(train_power)
nnet_forecast <- forecast(nnet_model, h = length(test_power))
nnet_rmse <- sqrt(mean((test_power - nnet_forecast$mean)^2, na.rm = TRUE))
# Print RMSE results for comparison
rmse_results <- data.frame(
Model = c("Auto ARIMA", "Neural Network"),
RMSE = c(arima_rmse, nnet_rmse)
)
print(rmse_results)
# Convert forecast results to data frames for ggplot2
# Auto ARIMA Data Frame
arima_forecast_df <- data.frame(
Date = time(arima_forecast$mean),
Forecast = arima_forecast$mean,
Lower80 = ifelse(!is.null(arima_forecast$lower), arima_forecast$lower[, "80%"], NA),
Upper80 = ifelse(!is.null(arima_forecast$upper), arima_forecast$upper[, "80%"], NA)
)
# Neural Network Data Frame (without confidence intervals)
nnet_forecast_df <- data.frame(
Date = time(nnet_forecast$mean),
Forecast = nnet_forecast$mean
)
# Plot both forecasts using ggplot2
ggplot() +
geom_line(data = arima_forecast_df, aes(x = Date, y = Forecast, color = "Auto ARIMA"), size = 1) +
geom_ribbon(data = arima_forecast_df, aes(x = Date, ymin = Lower80, ymax = Upper80), alpha = 0.2, fill = "blue") +
geom_line(data = nnet_forecast_df, aes(x = Date, y = Forecast, color = "Neural Network"), size = 1) +
labs(
title = "Forecast Comparison: Auto ARIMA vs Neural Network",
x = "Time",
y = "Forecasted Power Values"
) +
scale_color_manual(name = "Model", values = c("Auto ARIMA" = "blue", "Neural Network" = "red")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
library(ggplot2)
# Prepare RMSE results for comparison
rmse_results <- data.frame(
Model = c("Auto ARIMA", "Neural Network"),
RMSE = c(arima_rmse, nnet_rmse)
)
# Bar plot of RMSE values by each model
ggplot(rmse_results, aes(x = Model, y = RMSE, fill = Model)) +
geom_bar(stat = "identity", color = "black") +
labs(title = "RMSE Values by Model",
x = "Model",
y = "RMSE") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
ggplot() +
geom_line(data = arima_forecast_df, aes(x = Date, y = Forecast, color = "Auto ARIMA"), size = 1) +
geom_ribbon(data = arima_forecast_df, aes(x = Date, ymin = Lower80, ymax = Upper80), alpha = 0.2, fill = "blue") +
geom_line(data = nnet_forecast_df, aes(x = Date, y = Forecast, color = "Neural Network"), size = 1) +
labs(
title = "Forecast Comparison: Auto ARIMA vs Neural Network",
x = "Time",
y = "Forecasted Power Values"
) +
scale_color_manual(name = "Model", values = c("Auto ARIMA" = "blue", "Neural Network" = "red")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
